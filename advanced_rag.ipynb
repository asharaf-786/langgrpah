{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd441cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"unstructured[pdf]==0.10.19\" pillow pydantic lxml matplotlib tiktoken open_clip_torch torch\n",
    "# %pip install --upgrade \"onnxruntime==1.16.3\"\n",
    "# %pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f488697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/asharafansari/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/asharafansari/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab2163b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aad89c3ff44967bd86344300ec0bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f546daabdb421ca7ed10ff9e33e046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00ef655d6604a7899be91172beeee62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "OMP: Warning #96: Cannot form a team with 4 threads, using 1 instead.\n",
      "OMP: Hint Consider unsetting KMP_DEVICE_THREAD_LIMIT (KMP_ALL_THREADS), KMP_TEAMS_THREAD_LIMIT, and OMP_THREAD_LIMIT (if any are set).\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "image_path = \"/Users/asharafansari/Desktop/advanced_rag/images\"\n",
    "# Extract images, tables, and chunk text\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path,\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=image_path\n",
    ")\n",
    "\n",
    "# Categorize by type\n",
    "tables = []\n",
    "texts = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        tables.append(str(element))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        texts.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "load_dotenv() \n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=\"http://localhost:11434\", \n",
    "    model=\"nomic-embed-text:latest\"\n",
    ")\n",
    "\n",
    "if not pc.has_index(pinecone_index_name):\n",
    "    pc.create_index(\n",
    "        name=pinecone_index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    \n",
    "index = pc.Index(pinecone_index_name)\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f30c1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Text summary chain\n",
    "\n",
    "model = ChatOllama( \n",
    "    model = \"llama3.2:latest\",\n",
    "    base_url = \"http://localhost:11434\",\n",
    ")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Apply to text\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "\n",
    "# Apply to tables\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables):\n",
    "    \n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key\n",
    "    )\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "retriever_multi_vector = create_multi_vector_retriever(\n",
    "    vector_store,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f8a8e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a concise summary of the table:\\n\\n\"Wildland Fire Statistics by Year (2019-2022): \\n- Structures burned: 963, 17,904, 5,972, 2,717\\n- Residences affected: 46%, 54%, 60%, 46%\"Summary: Wildfire statistics from 2018-2022, indicating a significant increase in wildfires each year, with most being human-caused (89%), and larger lightning-caused fires burning more acreage (53%).\"Acres Burned by Wildfires\"\\n\\n* Acres burned: millions\\n* Number of fires: unknown (missing data)Here is a concise summary optimized for retrieval:\\n\\n**Wildfire Statistics by Region:**\\n\\n* **Western US:** Larger wildfires burn more acreage, with over 20,000 fires burning approximately 5.8 million acres in 2022.\\n* **Eastern US:** More fires occur on non-federal lands, with 85% (1.5 million acres) of burned acreage on private land.\\n\\nThis summary captures the key findings and regional differences in wildfire statistics, making it easily searchable for specific information.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"How many % Wildfire Damages in Structures Burned is there for the year 2019?\"\"\"\n",
    "docs = vector_store.similarity_search(query)\n",
    "page_contents=''\n",
    "for doc in docs:\n",
    "    page_contents+=doc.page_content\n",
    "page_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "28f924f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt_text = ChatPromptTemplate.from_template(template)\n",
    "chain = rag_prompt_text | model | StrOutputParser() \n",
    "response = chain.invoke({\"context\":page_contents,\"question\":query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "173e5f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the year 2019, according to the provided context, 46% of structures burned were damaged by wildfires.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgrpah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
